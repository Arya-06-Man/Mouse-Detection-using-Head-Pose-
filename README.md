# Mouse-Detection-using-Head-Pose-
This project is a hands-free mouse controller built with the MediaPipe framework. It tracks a user's head pose for stable cursor navigation and detects hand gestures—such as a fist for clicking and a pinch for scrolling —all using a standard webcam
This project lets you control your computer's mouse entirely hands-free. Using just a standard webcam, it tracks your head movements to navigate the cursor and recognizes hand gestures for actions.We built this system on the MediaPipe framework and use the Perspective-n-Point (PnP) algorithm to create a stable, low-latency controller. It's designed to be robust even with a regular, low-resolution webcam, making it a practical solution for assistive technology.Key FeaturesHead Pose Navigation: Control the cursor just by moving your head—looking left/right (yaw) and up/down (pitch).Gesture-Based Actions:Fist to Click: Make a fist to trigger a mouse click.Pinch to Scroll: Pinch your thumb and index finger together to scroll up and down.Framework Comparison: We include a detailed test showing why MediaPipe is objectively a better choice than the classic Dlib library for this real-time task.High Performance: The final system is lightweight and optimized to run smoothly in real-time.The ProblemOur first idea was to track the user's eyes (pupil tracking) and detect blinks for clicks. This approach failed immediately when we used a standard laptop webcam.Unstable Gaze: The video quality was just too low and noisy. It was impossible to get a stable lock on the pupil, which resulted in an unusable, "jittery" cursor that jumped erratically all over the screen.Unreliable Clicks: The low-fidelity video also made it hard for the system to tell an intentional, long blink from a normal one. This led to a frustrating experience with constant false clicks and missed inputs.The Solution: Head Pose + GesturesWe pivoted to a much more robust two-part system:1. Head Pose for NavigationInstead of tracking the unreliable iris, we track the head's overall orientation. This is a far more stable proxy for where you're looking.Here’s the breakdown:Landmark Detection: We use MediaPipe FaceMesh to find 468 3D facial landmarks in real-time.Point Selection: We only need 6 of those points: the nose tip, eye corners, ears, and chin.PnP Algorithm: We map those 6 2D points from the webcam to a generic 3D head model. Using some camera math, we then use OpenCV's cv2.solvePnP function to figure out the head's 3D orientation in space.Angle Calculation: This process gives us a rotation matrix, which we convert into simple Euler angles (yaw, pitch, roll). These angles tell us exactly which way the head is pointing.Smoothing: The raw angle data can be a little shaky. We pass the yaw and pitch values through an Exponential Moving Average (EMA) filter to smooth out any micro-jitters. This is what creates the stable and fluid cursor motion.2. Hand Gestures for ActionsTo replace the failed blink detection, we integrated MediaPipe Hands.Fist Detection (Click): The code checks the landmarks of the hand. If it sees that all four fingertips (index, middle, ring, pinky) are positioned below their corresponding middle knuckles, it registers a "fist" and triggers a mouse click.Pinch Detection (Scroll): We constantly calculate the distance between the tip of the thumb (Landmark 4) and the index finger (Landmark 8). If that distance drops below a certain threshold, the system activates "scroll mode." Moving your pinched hand up or down then scrolls the page.Framework Comparison: Dlib vs. MediaPipeA big part of this project was finding the right tool for the job. We did a head-to-head comparison of the classic Dlib library and the modern MediaPipe framework. The results weren't even close.FeatureDlib (Classic ML)MediaPipe (Modern CNN)Core TechHOG + Regression TreesLightweight CNNPerformance14-30 FPS100-135 FPSRobustnessCritical Failure. Fails completely when the head is turned, as it only detects frontal faces.Highly Robust. Tracks varied head poses and lighting conditions without issue.ConclusionUnsuitable for a real-time, practical application.Objectively superior in every metric.(placeholder: You can add Fig. 3 and Fig. 4 as a single image here)Technology StackThis project was built with a few key Python libraries:OpenCV (cv2): For all the camera I/O, image manipulation, and the solvePnP algorithm.MediaPipe (mediapipe): For the high-performance FaceMesh and Hand tracking models.NumPy (numpy): For all the numerical operations, vector/matrix math, and the smoothing filter.PyAutoGUI (pyautogui): To programmatically control the system's mouse cursor and trigger clicks/scrolls.Dlib (dlib): Used only for the comparative analysis.Installation & UsageClone the repository:Bashgit clone https://github.com/BlackBetty-SaysHello/Mouse-Detection-using-Head-Pose-.git
cd Mouse-Detection-using-Head-Pose-
Install dependencies:Bashpip install opencv-python mediapipe numpy pyautogui dlib
Download Dlib Model (for comparison script):If you want to run the Dlib comparison, you'll need its pre-trained model.Download shape_predictor_68_face_landmarks.dat from Dlib's website.Unzip it and place the .dat file in the same folder as the scripts.Run the Scripts:To run the Dlib vs. MediaPipe comparison:(This is based on Listing 1 in our paper)Bashpython head_pose_comparison.py
(You can edit the USE_MEDIAPIPE = False flag in the code to switch between them)To run the final, integrated application:(This is based on Listing 2 in our paper)Bashpython gaze_controller.py
Challenges & Future WorkKey Challenges (What We Learned)Initial Concept Failure: Our first idea (eye tracking) was a total flop. Pivoting to head-pose estimation was the right move and the key to the project's success.Dlib's Non-Frontal Failure: We were surprised to find Dlib's get_frontal_face_detector() was unusable. It stops tracking the instant a user turns their head, which makes it useless for this kind of application.Integration FPS Drop: The final integrated system (gaze_controller.py) runs two deep-learning models (FaceMesh and Hands) on every single frame. This is very demanding and causes a significant performance drop (from ~120 FPS down to 5-15 FPS). This is a critical trade-off between having gesture features and real-time performance.Future Work (What's Next?)Optimize the FPS Drop: We need to investigate ways to improve performance. Maybe run the hand detection in a separate thread, or only check for a gesture every Nth frame instead of constantly.Dynamic Screen Mapping: It would be great to add a "calibration" step that lets the system learn a user's specific range of head motion and map it to their screen.YOLOv8 Integration: Our paper mentions this, but it would be interesting to try integrating a custom-trained YOLOv8 model for fist detection to see if it's more lightweight than the full MediaPipe Hands model.AuthorsThis project was developed by:Aryaman OberoiChandraveer SisodiaShivam KumarAnikait Dixit(All from IIT Kanpur)
